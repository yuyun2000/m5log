模型微调的方法有两种：
1、数据上传至对应的ai公司，他们微调完之后给你相关的api，你像调用豆包一样来调用你微调后的模型，比如：豆包，openai，google等
2、使用开源的模型，在自己的服务器上进行微调，模型和数据都在本地服务器，不会外传至其他地方，微调完成后在本地服务器上运行模型，后续成本只有电费（软硬件维护费不算），比如：qwen、llama、grok等

## 闭源方案：
豆包
![](Pasted%20image%2020260104093210.png)
openai
![](Pasted%20image%2020260104093940.png)
谷歌gemini：
![](Pasted%20image%2020260104100354.png)

## 开源方案
只要模型是开源的（Deepseek、百度ernie、谷歌Gemma、openai-gptoss等等，有的公司不仅提供闭源模型的在线微调，也有开源模型放出来供开发者进行离线微调），都可以用来微调，但是不同参数大小的模型微调一次的成本不同，简单来说对比的表格如下：
![](Pasted%20image%2020260104100847.png)
比如70B的模型，使用LoRa微调，需要160GB显存，对应的服务器购入价格：
A100   显存 80G  价格  17-18w
H100  显存  80G  价格 21w   
H200  显存  141 GB  价格 28w

比如 2xA100价格约 18w x 2 + 10 = 46w左右，不同方案可能会便宜很多，我不是卖这个的，了解的不多...
发论文这种没有长期商业化需求的，建议租服务器（2xA100一个月8000左右）或者直接使用在线微调的方式进行，前提是确保数据安全



