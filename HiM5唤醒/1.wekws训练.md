## 把目录下所有文件按行写入scp文件

```
audio_dir="/hy-tmp/himfive"
label="<HI>"

find "$audio_dir" -type f \( -iname "*.wav" -o -iname "*.flac" \) \
| while read -r f; do
    id="${f##*/}"              # 去掉前面的路径
    id="${id%.*}"               # 去掉扩展名
    echo "$id $f"
    echo "$id $label" >&3
done > wav1.scp 3> text1
```

```
audio_dir="/hy-tmp/last_neg"
label="<FILLER>"

find "$audio_dir" -type f \( -iname "*.wav" -o -iname "*.flac" \) \
| while read -r f; do
    id="${f##*/}"              # 去掉前面的路径
    id="${id%.*}"               # 去掉扩展名
    echo "$id $f"
    echo "$id $label" >&3
done > wav2.scp 3> text2
```

```
audio_dir="/hy-tmp/musan"
label="<FILLER>"

find "$audio_dir" -type f \( -iname "*.wav" -o -iname "*.flac" \) \
| while read -r f; do
    id="${f##*/}"              # 去掉前面的路径
    id="${id%.*}"               # 去掉扩展名
    echo "$id $f"
    echo "$id $label" >&3
done > noi.scp 3> temp
```

### 多个合为一个
```
cat *.scp | sort -u > wav.scp && cat text* | sort -u > text
```

### 一个文件随机拆分
```
paste wav.scp text | shuf | awk -v n=$(wc -l < wav.scp) 'BEGIN{train_w="train.scp"; test_w="test.scp"; train_t="train.text"; test_t="test.text"} {split($0,a,"\t"); if(NR<=n*0.95){print a[1] > train_w; print a[2] > train_t} else {print a[1] > test_w; print a[2] > test_t}}'
```


## 生成噪声 混响增强
```
python tools/make_lmdb.py /hy-tmp/noi.scp /hy-tmp/noi-lmdb
```

```
python tools/make_lmdb.py /hy-tmp/rir.scp /hy-tmp/rir-lmdb
```

```
sudo apt-get install sox libsox-dev
```
## run.sh
```
#!/bin/bash
# Copyright 2021  Binbin Zhang(binbzha@qq.com)

. ./path.sh

stage=1
stop_stage=4
num_keywords=1

config=conf/mdtc-fbank.yaml
norm_mean=true
norm_var=true
gpus="0"

checkpoint=
dir=exp/mdtc

noise_lmdb=/hy-tmp/noi-lmdb
reverb_lmdb=/hy-tmp/rir-lmdb

num_average=30
score_checkpoint=$dir/avg_${num_average}.pt

download_dir=./data/local # your data dir

. tools/parse_options.sh || exit 1;
window_shift=50

if [ ${stage} -le -1 ] && [ ${stop_stage} -ge -1 ]; then
  echo "Download and extracte all datasets"
  local/mobvoi_data_download.sh --dl_dir $download_dir
fi


if [ ${stage} -le 0 ] && [ ${stop_stage} -ge 0 ]; then
  echo "Preparing datasets..."
  mkdir -p dict
  echo "<FILLER> -1" > dict/dict.txt
  echo "<HI_XIAOWEN> 0" >> dict/dict.txt
  echo "<NIHAO_WENWEN> 1" >> dict/dict.txt
  awk '{print $1}' dict/dict.txt > dict/words.txt

  for folder in train dev test; do
    mkdir -p data/$folder
    for prefix in p n; do
      mkdir -p data/${prefix}_$folder
      json_path=$download_dir/mobvoi_hotword_dataset_resources/${prefix}_$folder.json
      local/prepare_data.py $download_dir/mobvoi_hotword_dataset $json_path \
        dict/dict.txt data/${prefix}_$folder
    done
    cat data/p_$folder/wav.scp data/n_$folder/wav.scp > data/$folder/wav.scp
    cat data/p_$folder/text data/n_$folder/text > data/$folder/text
    rm -rf data/p_$folder data/n_$folder
  done
fi


if [ ${stage} -le 1 ] && [ ${stop_stage} -ge 1 ]; then
  echo "Compute CMVN and Format datasets"
  tools/compute_cmvn_stats.py --num_workers 16 --train_config $config \
    --in_scp data/train/wav.scp \
    --out_cmvn data/train/global_cmvn

  for x in train dev; do
    tools/wav_to_duration.sh --nj 8 data/$x/wav.scp data/$x/wav.dur
    tools/make_list.py data/$x/wav.scp data/$x/text \
      data/$x/wav.dur data/$x/data.list
  done
fi


if [ ${stage} -le 2 ] && [ ${stop_stage} -ge 2 ]; then
  echo "Start training ..."
  mkdir -p $dir
  cmvn_opts=
  $norm_mean && cmvn_opts="--cmvn_file data/train/global_cmvn"
  $norm_var && cmvn_opts="$cmvn_opts --norm_var"
  num_gpus=$(echo $gpus | awk -F ',' '{print NF}')
  torchrun --standalone --nnodes=1 --nproc_per_node=$num_gpus \
    wekws/bin/train.py --gpus $gpus \
      --config $config \
      --train_data data/train/data.list \
      --cv_data data/dev/data.list \
      --model_dir $dir \
      --num_workers 8 \
      --num_keywords $num_keywords \
      --min_duration 50 \
      --seed 666 \
      --dict ./dict \
      $cmvn_opts \
      ${reverb_lmdb:+--reverb_lmdb $reverb_lmdb} \
      ${noise_lmdb:+--noise_lmdb $noise_lmdb} \
      ${checkpoint:+--checkpoint $checkpoint}
fi

if [ ${stage} -le 3 ] && [ ${stop_stage} -ge 3 ]; then
  echo "Do model average, Compute FRR/FAR ..."
  python wekws/bin/average_model.py \
    --dst_model $score_checkpoint \
    --src_path $dir  \
    --num ${num_average} \
    --val_best
  result_dir=$dir/test_$(basename $score_checkpoint)
  mkdir -p $result_dir
  python wekws/bin/score.py \
    --config $dir/config.yaml \
    --test_data data/test/data.list \
    --gpu 0 \
    --batch_size 256 \
    --checkpoint $score_checkpoint \
    --score_file $result_dir/score.txt \
    --dict ./dict \
    --num_workers 8

  for keyword in `tail -n +2 dict/words.txt`; do
    python wekws/bin/compute_det.py \
      --keyword $keyword \
      --test_data data/test/data.list \
      --window_shift $window_shift \
      --score_file $result_dir/score.txt \
      --stats_file $result_dir/stats.${keyword}.txt
  done

  # plot det curve
  python wekws/bin/plot_det_curve.py \
      --keywords_dict dict/dict.txt \
      --stats_dir  $result_dir \
      --xlim 2 \
      --x_step 1 \
      --ylim 5 \
      --y_step 1 \
      --figure_file $result_dir/det.png
fi


if [ ${stage} -le 4 ] && [ ${stop_stage} -ge 4 ]; then
  jit_model=$(basename $score_checkpoint | sed -e 's:.pt$:.zip:g')
  onnx_model=$(basename $score_checkpoint | sed -e 's:.pt$:.onnx:g')
  python wekws/bin/export_jit.py \
    --config $dir/config.yaml \
    --checkpoint $score_checkpoint \
    --jit_model $dir/$jit_model
  python wekws/bin/export_onnx.py \
    --config $dir/config.yaml \
    --checkpoint $score_checkpoint \
    --onnx_model $dir/$onnx_model
fi
```

mdtc-fbank.yaml
```
dataset_conf:
    filter_conf:
        max_length: 2048
        min_length: 0
    resample_conf:
        resample_rate: 16000
    speed_perturb: false
    reverb_prob: 0.2
    noise_prob: 0.5
    feats_type: 'fbank'
    fbank_conf:
        num_mel_bins: 80
        frame_shift: 10
        frame_length: 25
        dither: 1.0
    spec_aug: true
    spec_aug_conf:
        num_t_mask: 1
        num_f_mask: 1
        max_t: 20
        max_f: 40
    shuffle: true
    shuffle_conf:
        shuffle_size: 1500
    sort: false
    batch_conf:
        batch_size: 256

model:
    hidden_dim: 32
    preprocessing:
        type: linear
    backbone:
        type: mdtc
        num_stack: 3
        stack_size: 3
        kernel_size: 5
        hidden_dim: 32
        causal: True

optim: adam
optim_conf:
    lr: 0.001
    weight_decay: 0.00005

training_config:
    grad_clip: 5
    max_epoch: 20
    log_interval: 10
    criterion: max_pooling
```


dict/dict.txt
```
<HI> 0
<FILLER> -1
```
dict/words.txt
```
<HI>
<FILLER>
```



