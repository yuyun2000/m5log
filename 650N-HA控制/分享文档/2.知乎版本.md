# 我把微调后的Qwen模型部署到M5Stack上了，实测对话效果提升明显

前两天刚完成Qwen2.5-0.5B的微调训练，拿到了safetensor格式的模型文件。说实话，看着自己训练出来的模型躺在Hugging Face仓库里（`https://huggingface.co/yunyu1258/qwen2.5-0.5b-ha/tree/main`）
心里还挺有成就感的。但问题来了——**这玩意儿怎么部署到M5Stack设备上跑起来？**

折腾了一下午，踩了几个坑，最终成功把微调模型部署上去了。整个流程记录下来，希望能帮到同样想在边缘设备上跑自己模型的朋友。

---

## 💡 核心思路：safetensor → axmodel → 设备部署

M5Stack设备用的是AX650芯片，模型格式需要是axmodel，而我们微调完拿到的是safetensor。所以整个流程就是：

**safetensor格式模型 → 用pulsar2工具转换 → 得到axmodel → 替换设备上的预训练模型 → 重启服务**

听起来简单，但每一步都有细节要注意。

---

## 🔧 Step1: 搭建pulsar2转换环境

pulsar2是爱芯元智提供的模型转换工具，专门用来把各种AI模型转成AX系列芯片能用的axmodel格式。

### 安装过程
```bash
# 具体安装步骤可以参考官方文档
# 或者直接问M5Stack的AI助手：chat.m5stack.com
```

![pulsar2安装界面](../../file/Pasted%20image%2020251230101449.png)

这个AI助手挺好用的，**它连接了M5Stack所有对外的技术知识库**，安装过程中遇到环境问题、依赖冲突之类的，直接问它基本都能解决。

安装完成后会得到一个Docker环境，后续的模型转换都在这个环境里进行。

---

## ⚙️ Step2: 模型格式转换（关键步骤）

进入Docker环境后，运行这条命令：

```bash
pulsar2 llm_build \
  --input_path Qwen/qwen2.5-0.5b-ha \
  --output_path Qwen/qwen2.5-0.5B-p1024-ha-ax650 \
  --hidden_state_type bf16 \
  --prefill_len 128 \
  --kv_cache_len 1280 \
  --last_kv_cache_len 128 \
  --last_kv_cache_len 512 \
  --last_kv_cache_len 1024 \
  --chip AX650 \
  --parallel 24
```

### 参数说明
- `--input_path`：你的safetensor模型路径
- `--output_path`：转换后的axmodel输出路径（可以自己定义）
- `--hidden_state_type bf16`：使用BF16精度，平衡性能和内存占用
- `--prefill_len 128`：预填充长度，影响首次响应速度
- `--kv_cache_len`系列参数：KV缓存配置，决定模型能处理的上下文长度
- `--chip AX650`：指定目标芯片型号
- `--parallel 24`：并行度设置

转换过程大概需要几分钟到十几分钟不等，取决于模型大小。**转换完成后会得到一系列axmodel文件**，这些就是AX650芯片能直接加载的模型格式。

---

## 🔄 Step3: 替换设备上的模型

在替换模型之前，需要先把M5Stack设备上的LLM运行环境配置好。

### 环境准备
按照官方文档（或问AI助手）安装：
- `llm-sys`：底层系统服务
- `llm-llm`：模型推理引擎
- `openaiapi`：OpenAI兼容接口

然后安装官方的qwen2.5-0.5b预训练模型，**先确保这套环境能正常工作**，能通过OpenAI API调用成功。

### 模型替换
找到设备上的模型存储路径（通常在 `/opt/m5stack/data/qwen2.....`），把这个目录下的官方模型文件**用你转换好的axmodel替换掉**。

**⚠️ 注意：替换前最好备份一下原始模型，万一出问题还能回滚。**

---

## 🚀 Step4: 重启服务并验证

替换完模型后，重启LLM服务：

```bash
systemctl restart llm-sys
```

然后就可以通过OpenAI API接口发起对话了。**这时候调用的就是你微调后的模型**。

我测试了几轮对话，明显感觉模型在我微调的领域（家庭助理相关）回答更准确了，这就是微调的效果体现。

---

## 🎯 M5Stack在这套流程中的价值

整个部署过程下来，最大的感受是**M5Stack的工具链和生态做得真的挺完善**：

1. **pulsar2工具非常好用**：模型转换一条命令搞定，支持的参数配置也很灵活，能根据实际硬件资源调优
2. **官方AI助手chat.m5stack.com是真香**：连接了全套技术文档，遇到问题基本秒回，比翻文档效率高太多
3. **OpenAI兼容接口**：部署完直接用标准API调用，不用改现有的应用代码，这点对开发者很友好

对比其他边缘AI方案，M5Stack的优势在于**开箱即用的完整方案** —— 从硬件到软件，从模型转换到部署运行，一条龙服务都有。这对想快速落地AI应用的开发者来说省了太多事儿。

---

## 📝 最后

整个流程走下来，核心就是三步：**环境搭建 → 模型转换 → 替换部署**。过程中遇到任何问题，都可以问官方AI助手，基本都能得到解答。

下次有时间我打算测一下多轮对话的效果，看看KV Cache的配置对实际使用体验的影响有多大。如果你也在尝试在边缘设备上跑自己的微调模型，欢迎交流踩坑经验~ 🤝