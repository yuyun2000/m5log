# 实测：68分钟把Stable Diffusion 1.5部署到M5Stack-LLM 8850，单张图生成仅需9.5秒

**先说结论：使用M5Stack-LLM 8850加速卡，成功将SD1.5模型端侧部署，实现了文本嵌入0.7s + 20步扩散8s + 解码0.7s的推理速度，整个转换过程的主要耗时在模型量化环节（68分钟）。**

最近在折腾SD1.5的边缘设备部署，发现M5Stack-LLM 8850这块板子的算力其实挺适合跑扩散模型的。整个部署流程踩了些坑，这里记录一下完整过程，所有代码都开源在：`https://github.com/yuyun2000/sd15-to-ax8850-deploy`

---

## 一、模型选择：从Civitai找一个靠谱的SD1.5权重

登录 `https://civitai.com`，随便挑了个下载量还不错的模型做测试：

![](Pasted%20image%2020251126113503.png)

**选模型时注意三点：**
- Base Model必须是SD1.5（这个是硬性要求）
- 模型类型得是完整的Checkpoint（LoRA理论上也能转，但流程更复杂）
- 文件大小最好在1.99GB左右（大部分都是这个size，稍微大点问题不大）

我选的是xxmix9realistic模型，下载得到 `xxmix9realistic_v40.safetensors` 文件。

---

## 二、本地验证：确保模型能正常推理

在PC环境里先跑一遍，验证权重文件没问题：

```python
python safetensor_infer.py \
    --input_path "./s/xxmix9realistic_v40.safetensors" \
    --output_path "./test_output" \
    --negative_prompt "easynegative,ng_deepnegative_v1_75t,(worst quality:2),(low quality:2)..." \
    --prompt "1girl, upper body, (huge Laughing),sweety,sun glare, bokeh..."
```

推理出来的效果：

![](test_dpm_no_lora_steps_20%203.jpg)

**这一步主要是确认safetensor权重加载没问题**，后面转换ONNX时如果出幺蛾子，至少能排除是原始权重的锅。

---

## 三、导出ONNX：为AX8850转换做准备

```python
python export_onnx.py \
    --input_path ./s/xxmix9realistic_v40.safetensors \
    --output_path ./out_onnx \
    --isize 480x320 \
    --prompt "1girl, upper body, (huge Laughing)..."
```

**关于分辨率的说明：**
- `320x480` 是横图，`480x320` 是竖图
- 也可以设置 `512x512`，但再大就会报显存不足
- 这里的尺寸和后续VAE处理时的尺寸要对应（有8倍缩放关系）

⚠️ **注意：** 这里我没有导出Text Encoder模型，原因有二：
1. AX官方的导出demo环境有问题，一直导不出来
2. Text Encoder用的就是标准CLIP，各个模型都一样，没必要重复导出

---

## 四、ONNX推理验证（可选步骤）

这一步不是必须的，主要是验证ONNX导出的准确性：

```python
python dpm_infer.py
```

记得修改代码中对应的提示词和路径。推理结果：

![](txt2img_output_onnx%206.png)

对比原始safetensor的输出，效果基本一致，说明ONNX导出没问题。

---

## 五、准备矫正集：量化的关键一步

```python
python prepare_data.py
```

运行后会在目录下生成转换需要的矫正集（calibration data）。

**重点：** 不同的模型需要不同的矫正样本，这个脚本里的提示词需要根据你的模型特点自行调整，否则量化后的精度可能会有损失。

---

## 六、转换为AX模型：最耗时的环节

切换到Pulsar环境（我用的是4.2版本）：

### 6.1 转换UNet（⏱️ 重点耗时）

```bash
pulsar2 build \
    --input out_onnx/unet_sim_cut.onnx \
    --config config_unet_u16.json \
    --output_dir output_unet_ax \
    --output_name unet.axmodel
```

**这一步非常慢，我实测花了68分钟。** 建议挂后台跑，别盯着看进度条（会怀疑人生）。

### 6.2 转换VAE Decoder

```bash
pulsar2 build \
    --input out_onnx/vae_decoder_sim.onnx \
    --config config_vae_decoder_u16.json \
    --output_dir output_vae_ax \
    --output_name vae.axmodel
```

这个相对快很多。

### 6.3 Text Encoder模型

由于前面没导出ONNX，这里可以直接从HuggingFace下载我转好的：
```
https://huggingface.co/yunyu1258/SD1.5-AX8850-xxmix9realistic
```

这个仓库里也有完整的转换好的模型，以及在板子上推理的Python脚本，可以直接拿来跑。

---

## 七、板端实测：M5Stack-LLM 8850 的实际表现

最终在M5Stack-LLM 8850上的推理耗时：

| 阶段 | 耗时 |
|------|------|
| 文本嵌入（Text Encoder） | 0.7s |
| 20步扩散（UNet × 20） | 8s (单步0.4s) |
| 图像解码（VAE Decoder） | 0.7s |
| **总计** | **9.5s** |

生成效果：

![](92069eb4-c929-4999-b239-10ddf0c24990.png)

💡 **优化建议：** 可以尝试调整为10步推理，实测画质影响不大，生成速度能缩短到5秒左右。

---

## 写在最后

整个部署流程最大的坑在于：
- UNet量化真的慢（68分钟），需要提前规划时间
- 矫正集的质量直接影响量化精度，不能随便糊弄
- 分辨率的8倍关系要注意，各个模块要对应上

M5Stack-LLM 8850这块板子跑SD1.5还是挺实用的，单张图9.5秒的速度在边缘设备里算不错了。如果你也在做类似部署，可以参考这个流程，避免重复踩坑。

**完整代码和转换好的模型：**
- GitHub: `https://github.com/yuyun2000/sd15-to-ax8850-deploy`
- 模型下载: `https://huggingface.co/yunyu1258/SD1.5-AX8850-xxmix9realistic`

有问题欢迎评论区讨论~