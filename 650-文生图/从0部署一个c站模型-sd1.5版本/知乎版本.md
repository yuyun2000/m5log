# 从Civitai到M5Stack：3小时让SD1.5模型跑上8850加速卡⚡

> **直接说结论**：完整的SD1.5模型（1.99GB）可以成功部署到M5Stack-LLM 8850加速卡上，单张图生成耗时约9.5秒（20步采样），10步采样可压缩到5秒内，画质损失可以接受。整个部署流程我踩了3天坑，这篇文章帮你避开所有雷区。

---

## 💡 为什么要折腾这个？

市面上SD部署方案一抓一大把，但能在**边缘设备**上跑完整checkpoint的不多。M5Stack-LLM 8850这块板子给了我一个新思路：

- **成本可控**：相比动辄上万的工作站，这个方案更适合小批量产品原型
- **功耗友好**：实测推理功耗不到20W，能塞进嵌入式场景
- **生态完善**：M5Stack的Arduino生态让后续集成更简单

当然，**限制也很明显**：最大支持512x512分辨率，模型必须是SD1.5（SDXL别想了）。但对于特定场景（比如工业检测、边缘AIGC设备），这些约束完全可以接受。

---

## 🎯 部署核心流程（3步走）

### 第一步：选对模型是成功的一半

跑去Civitai随便下个模型？**90%会翻车**。必须满足3个硬性条件：

| 检查项 | 要求 | 为什么重要 |
|--------|------|-----------|
| Base Model | 必须是SD1.5 | 其他版本架构不兼容 |
| 模型类型 | Checkpoint完整权重 | LoRA单独处理太麻烦 |
| 文件大小 | 1.99GB左右 | 大幅偏差可能有问题 |

我用的测试模型：[xxmix9realistic_v40](https://civitai.com/models/47274/xxmix9realistic)，下载后得到`.safetensors`文件。

**避坑指南**：Civitai上很多模型标注的Base Model是错的，下载前先看评论区有没有人吐槽版本问题。

---

### 第二步：PC端验证→导出ONNX（务必先验证！）

很多人上来就转模型，结果发现板子上跑不出图才来查原因。**正确姿势是先在PC上验证原始权重能用**：

```bash
python safetensor_infer.py \
    --input_path "./xxmix9realistic_v40.safetensors" \
    --output_path "./test_output" \
    --prompt "1girl, upper body, huge laughing, bokeh..."
```

![实测生成效果](test_dpm_no_lora_steps_20%203.jpg)
*▲ PC端原始权重推理结果，用来做后续对比基准*

确认没问题后导出ONNX：

```bash
python export_onnx.py \
    --input_path ./xxmix9realistic_v40.safetensors \
    --output_path ./out_onnx \
    --isize 480x320  # 竖图尺寸，横图用320x480
```

**关键参数说明**：
- `--isize`：必须是8的倍数（VAE有8倍下采样），超过512会OOM
- `--prompt`：从模型示例图复制一个，用来验证转换准确性

这里有个**隐藏的坑**：text_encoder我没导出，原因有二：
1. 爱芯官方的导出demo环境有bug（试了3次都失败）
2. 所有SD1.5模型的text_encoder都是标准CLIP，没必要重复转

可以直接用现成的：[我打包好的text_encoder.axmodel](https://huggingface.co/yunyu1258/SD1.5-AX650-Dark_Sushi_Mix)

---

### 第三步：ONNX→AXModel（重点来了）

**环境切换**：这里需要用爱芯的Pulsar2工具链（版本4.2），和之前的Python环境分开。

#### 1. 准备矫正数据集（容易被忽略）

```bash
python prepare_data.py
```

这步生成的是量化校准数据，**不同模型需要不同的样本**。我的做法是：
- 从目标模型的示例图中提取10个prompt
- 生成对应的latent分布作为校准集

#### 2. 转换UNet（最慢的环节）

```bash
pulsar2 build \
    --input out_onnx/unet_sim_cut.onnx \
    --config config_unet_u16.json \
    --output_dir output_unet_ax \
    --output_name unet.axmodel
```

**心理准备**：这步巨慢，我的台式机（i7-12700）跑了68分钟。期间CPU占用100%，内存峰值38GB。

⚠️ **如果卡在这里**：
- 检查ONNX模型是否被`onnxsim`优化过（必须的）
- 确认config文件中量化策略是u16（u8会糊）
- 矫正集数据量不要少于10条

#### 3. 转换VAE Decoder（快很多）

```bash
pulsar2 build \
    --input out_onnx/vae_decoder_sim.onnx \
    --config config_vae_decoder_u16.json \
    --output_dir output_vae_ax \
    --output_name vae.axmodel
```

这个只要5分钟左右。

---

## 🚀 板子上实测（性能数据）

把3个`.axmodel`文件传到M5Stack-LLM 8850上，运行测试脚本：

```python
# 完整代码见：https://github.com/yuyun2000/sd15-to-ax8850-deploy
python ax_inference.py --steps 20
```

**实测性能拆解**：

| 阶段 | 耗时 | 备注 |
|------|------|------|
| Text Encoder | 0.7s | 固定值，和prompt长度无关 |
| UNet扩散(20步) | 8.0s | 单步0.4s，可以减步数 |
| VAE Decoder | 0.7s | 固定值 |
| **总计** | **9.5s** | 单张480x320图像 |

![板子实测效果](92069eb4-c929-4999-b239-10ddf0c24990.png)
*▲ 10步采样的生成结果，和PC端20步基本一致*

**优化建议**：
- 降到10步采样，总耗时压到5秒，画质肉眼看不出差别
- 如果能接受512x512，性能会更好（latent计算量更小）
- 批量生图时可以复用text_encoder的结果

---

## 🔥 踩过的坑（血泪教训）

### 坑1：ONNX导出后模型巨大（>10GB）
**原因**：没有做图优化和常量折叠  
**解法**：必须先用`onnxsim`简化，参数`--skip-fuse-bn`别漏

### 坑2：UNet转换到一半就OOM
**原因**：矫正集数据太多或者batch size设置错误  
**解法**：矫正集控制在10-20条，config里`calibration_batch_size=1`

### 坑3：生成的图全是噪点
**原因**：量化策略用了u8（精度不够）  
**解法**：所有config都改成u16，虽然模型大一倍但必须的

### 坑4：VAE输出颜色偏绿
**原因**：归一化参数没对齐  
**解法**：检查mean/std是否和训练时一致（我在代码里写死了）

---

## 💬 适合谁用？不适合谁？

**适合的场景**：
- 边缘设备AIGC应用（店铺营销屏、展会互动装置）
- 工业视觉的辅助生成（缺陷模拟、数据增强）
- 原型验证（先在小板子上跑通再上云）

**不适合的场景**：
- 追求极致画质（SDXL、分辨率>512）
- 需要实时交互（9秒延迟还是太高）
- 复杂的ControlNet流程（板子算力撑不住）

---

## 📦 完整代码和模型

所有脚本已开源：**https://github.com/yuyun2000/sd15-to-ax8850-deploy**

包含：
- 导出脚本（safetensor→ONNX）
- 量化配置文件
- 板端推理代码
- 预转好的text_encoder.axmodel

转好的完整模型（xxmix9realistic）：[HuggingFace链接](https://huggingface.co/yunyu1258/SD1.5-AX650-Dark_Sushi_Mix)

---

## 🤔 后续计划

这套方案还有几个方向可以优化：

1. **LoRA支持**：目前只能跑完整checkpoint，LoRA融合方案在研究中
2. **多模型切换**：板子上同时部署3-4个不同风格的模型，动态加载
3. **ControlNet适配**：简化版的Canny/Depth应该能塞进去

如果你也在折腾边缘AI部署，欢迎评论区交流，我会持续更新踩坑记录 🔧

---

**最后说一句**：这个方案绝不是"最优解"，但对于想快速验证"SD模型能不能上嵌入式设备"的人来说，M5Stack的这套工具链确实降低了不少门槛。比起从零搭建ONNX Runtime环境、手动写算子适配，直接用AXModel省了至少2周时间。

*（全文3500字，阅读约需8分钟，代码运行约需3小时）*